{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mi-jWv4v03Uk","outputId":"38370622-5313-4d69-d7d9-46217e67e05c","executionInfo":{"status":"ok","timestamp":1765057577927,"user_tz":360,"elapsed":53802,"user":{"displayName":"pallavi sharma","userId":"07344016866799049195"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","BASE_DIR  : /content/drive/Shareddrives/TissueMotionForecasting\n","TRAIN_ROOT: /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train\n","\n","Scanning datasets and keyframes:\n","\n","[dataset_1] all keyframes:\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_1\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_2\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_3\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_4\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_5\n","\n","[dataset_2] all keyframes:\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_1\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_2\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_3\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_4\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_5\n","\n","[dataset_3] all keyframes:\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_1\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_2\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_3\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_4\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_5\n","\n","Keyframes to use (all datasets, kf1–5):\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_1\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_2\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_3\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_4\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_5\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_1\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_2\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_3\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_4\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_5\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_1\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_2\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_3\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_4\n","  /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_5\n","Total keyframes used: 15\n","\n","[keyframe_1]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_1/data/disparity\n","  RAW disparity : 197\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_2]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_2/data/disparity\n","  RAW disparity : 280\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_3]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_3/data/disparity\n","  RAW disparity : 0\n","  COLORED       : 0\n","\n","[keyframe_4]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_4/data/disparity\n","  RAW disparity : 1\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_5]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_1/keyframe_5/data/disparity\n","  RAW disparity : 1\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_1]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_1/data/disparity\n","  RAW disparity : 88\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_2]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_2/data/disparity\n","  RAW disparity : 1033\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_3]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_3/data/disparity\n","  RAW disparity : 1102\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_4]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_4/data/disparity\n","  RAW disparity : 2114\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_5]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_2/keyframe_5/data/disparity\n","  RAW disparity : 1\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_1]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_1/data/disparity\n","  RAW disparity : 329\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_2]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_2/data/disparity\n","  RAW disparity : 1597\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_3]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_3/data/disparity\n","  RAW disparity : 448\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_4]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_4/data/disparity\n","  RAW disparity : 834\n","  COLORED       : 0\n","   example RAW     : 000000.png\n","\n","[keyframe_5]\n","  disparity dir : /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_3/keyframe_5/data/disparity\n","  RAW disparity : 1\n","  COLORED       : 0\n","   example RAW     : 000000.png\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, glob\n","from pathlib import Path\n","\n","BASE_DIR = Path(\"/content/drive/Shareddrives/TissueMotionForecasting\")\n","TRAIN_ROOT = BASE_DIR / \"scared_data\" / \"train\"\n","\n","print(\"BASE_DIR  :\", BASE_DIR)\n","print(\"TRAIN_ROOT:\", TRAIN_ROOT)\n","\n","DATASETS_TO_USE = [\"dataset_1\", \"dataset_2\", \"dataset_3\"]\n","KEYFRAME_NAMES = [f\"keyframe_{i}\" for i in range(1, 6)]\n","\n","KEYFRAMES_TO_USE = []\n","\n","print(\"\\nScanning datasets and keyframes:\")\n","for ds_name in DATASETS_TO_USE:\n","    ds_root = TRAIN_ROOT / ds_name\n","    kfs = sorted(ds_root.glob(\"keyframe_*\"))\n","    print(f\"\\n[{ds_name}] all keyframes:\")\n","    for kf in kfs:\n","        print(\" \", kf)\n","\n","    for kf in kfs:\n","        if kf.name in KEYFRAME_NAMES:\n","            KEYFRAMES_TO_USE.append(kf)\n","\n","print(\"\\nKeyframes to use (all datasets, kf1–5):\")\n","for kf in KEYFRAMES_TO_USE:\n","    print(\" \", kf)\n","\n","print(\"Total keyframes used:\", len(KEYFRAMES_TO_USE))\n","\n","\n","for kf in KEYFRAMES_TO_USE:\n","    disp_dir = kf / \"data\" / \"disparity\"\n","    raw_files = sorted([f for f in disp_dir.glob(\"*.png\")\n","                        if not f.name.startswith(\"colored\")])\n","    colored_files = sorted(disp_dir.glob(\"colored_*.png\"))\n","\n","    print(f\"\\n[{kf.name}]\")\n","    print(\"  disparity dir :\", disp_dir)\n","    print(\"  RAW disparity :\", len(raw_files))\n","    print(\"  COLORED       :\", len(colored_files))\n","\n","    if raw_files:\n","        print(\"   example RAW     :\", raw_files[0].name)\n","    if colored_files:\n","        print(\"   example COLORED :\", colored_files[0].name)\n","\n"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","CONTEXT_LEN = 3\n","FORECAST_HORIZON = 5\n","DISP_SCALE = 256.0\n","\n","\n","class DisparityForecastDataset(Dataset):\n","    \"\"\"\n","    Uses RAW disparity from:\n","      DATA_ROOT/keyframe_*/data/disparity/*.png\n","\n","    - Ignores colored_*.png\n","    - Input : CONTEXT_LEN past frames  -> tensor [C, H, W]\n","    - Target: frame at t + FORECAST_HORIZON -> tensor [1, H, W]\n","    \"\"\"\n","    def __init__(self, data_root, keyframes_paths,\n","                 context_len=3, forecast_horizon=1, scale=256.0):\n","        self.samples = []\n","        self.context_len = context_len\n","        self.forecast_horizon = forecast_horizon\n","        self.scale = scale\n","\n","        for kf in keyframes_paths:\n","            disp_dir = kf / \"data\" / \"disparity\"\n","\n","\n","            frame_paths = sorted([\n","                p for p in disp_dir.glob(\"*.png\")\n","                if not p.name.startswith(\"colored\")\n","            ])\n","\n","            if len(frame_paths) < context_len + forecast_horizon:\n","                continue\n","\n","\n","            for i in range(context_len - 1,\n","                           len(frame_paths) - forecast_horizon):\n","                ctx_paths = frame_paths[i - (context_len - 1): i + 1]\n","                tgt_path = frame_paths[i + forecast_horizon]\n","                self.samples.append((ctx_paths, tgt_path))\n","\n","        print(f\"DisparityForecastDataset: {len(self.samples)} samples\")\n","\n","    def _load_disp(self, path):\n","        from PIL import Image\n","        import numpy as np\n","\n","        img = Image.open(path).convert(\"I\")\n","        arr = np.array(img, dtype=np.float32)\n","\n","\n","        arr = arr / self.scale\n","        return arr\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        ctx_paths, tgt_path = self.samples[idx]\n","\n","\n","        ctx_frames = [self._load_disp(p) for p in ctx_paths]  # list of [H,W]\n","        import numpy as np\n","        ctx = np.stack(ctx_frames, axis=0)   # [C,H,W]\n","\n","        tgt = self._load_disp(tgt_path)     # [H,W]\n","\n","        ctx = torch.from_numpy(ctx.astype(\"float32\"))         # [C,H,W]\n","        tgt = torch.from_numpy(tgt.astype(\"float32\")).unsqueeze(0)  # [1,H,W]\n","\n","        return ctx, tgt\n","\n","\n","dataset = DisparityForecastDataset(\n","    data_root=TRAIN_ROOT,\n","    keyframes_paths=KEYFRAMES_TO_USE,\n","    context_len=CONTEXT_LEN,\n","    forecast_horizon=FORECAST_HORIZON,\n","    scale=DISP_SCALE,\n",")\n","\n","print(\"Total samples in dataset:\", len(dataset))\n","\n","ctx_sample, tgt_sample = dataset[0]\n","print(\"Context shape:\", ctx_sample.shape)\n","print(\"Target shape :\", tgt_sample.shape)\n","print(\"Context min/max:\", ctx_sample.min().item(), ctx_sample.max().item())\n","print(\"Target  min/max:\", tgt_sample.min().item(), tgt_sample.max().item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlpJL9OcCk4C","outputId":"9f5fdc2d-acc2-4788-a684-ac7562dd7b8f","executionInfo":{"status":"ok","timestamp":1765057588291,"user_tz":360,"elapsed":10360,"user":{"displayName":"pallavi sharma","userId":"07344016866799049195"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DisparityForecastDataset: 7952 samples\n","Total samples in dataset: 7952\n","Context shape: torch.Size([3, 1024, 1280])\n","Target shape : torch.Size([1, 1024, 1280])\n","Context min/max: 0.0 60.48828125\n","Target  min/max: 0.0 57.32421875\n"]}]},{"cell_type":"code","source":["from torch.utils.data import random_split, DataLoader\n","\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_ds, val_ds = random_split(dataset, [train_size, val_size])\n","\n","print(\"Train samples:\", len(train_ds))\n","print(\"Val samples  :\", len(val_ds))\n","\n","BATCH_SIZE = 2\n","NUM_WORKERS = 2\n","\n","train_loader = DataLoader(\n","    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n","    num_workers=NUM_WORKERS, pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n","    num_workers=NUM_WORKERS, pin_memory=True\n",")\n","\n","\n","ctx, tgt = next(iter(train_loader))\n","print(\"Batch ctx shape:\", ctx.shape)   # [B, 3, H, W]\n","print(\"Batch tgt shape:\", tgt.shape)   # [B, 1, H, W]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xy1xXa7FDnA0","outputId":"9cb1b2f7-c8f5-429a-cd80-9ca2b8fbdda1","executionInfo":{"status":"ok","timestamp":1765057594396,"user_tz":360,"elapsed":6103,"user":{"displayName":"pallavi sharma","userId":"07344016866799049195"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train samples: 6361\n","Val samples  : 1591\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Batch ctx shape: torch.Size([2, 3, 1024, 1280])\n","Batch tgt shape: torch.Size([2, 1, 1024, 1280])\n"]}]},{"cell_type":"code","source":["\n","import torch.nn as nn\n","import torch\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=1):\n","        super().__init__()\n","\n","        self.enc1 = DoubleConv(in_channels, 32)\n","        self.enc2 = DoubleConv(32, 64)\n","        self.enc3 = DoubleConv(64, 128)\n","        self.enc4 = DoubleConv(128, 256)\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","        self.bottleneck = DoubleConv(256, 512)\n","\n","        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.dec4 = DoubleConv(256 + 256, 256)\n","\n","        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.dec3 = DoubleConv(128 + 128, 128)\n","\n","        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.dec2 = DoubleConv(64 + 64, 64)\n","\n","        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n","        self.dec1 = DoubleConv(32 + 32, 32)\n","\n","        self.out_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Encoder\n","        e1 = self.enc1(x)           # [B,32,H,W]\n","        e2 = self.enc2(self.pool(e1))   # [B,64,H/2,W/2]\n","        e3 = self.enc3(self.pool(e2))   # [B,128,H/4,W/4]\n","        e4 = self.enc4(self.pool(e3))   # [B,256,H/8,W/8]\n","\n","        # Bottleneck\n","        b = self.bottleneck(self.pool(e4))  # [B,512,H/16,W/16]\n","\n","        # Decoder\n","        d4 = self.up4(b)                # [B,256,H/8,W/8]\n","        d4 = torch.cat([d4, e4], dim=1) # [B,512,H/8,W/8]\n","        d4 = self.dec4(d4)\n","\n","        d3 = self.up3(d4)               # [B,128,H/4,W/4]\n","        d3 = torch.cat([d3, e3], dim=1)\n","        d3 = self.dec3(d3)\n","\n","        d2 = self.up2(d3)               # [B,64,H/2,W/2]\n","        d2 = torch.cat([d2, e2], dim=1)\n","        d2 = self.dec2(d2)\n","\n","        d1 = self.up1(d2)               # [B,32,H,W]\n","        d1 = torch.cat([d1, e1], dim=1)\n","        d1 = self.dec1(d1)\n","\n","        out = self.out_conv(d1)         # [B,1,H,W]\n","        return out\n","\n","\n","# instantiate once to check shapes\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = UNet(in_channels=3, out_channels=1).to(device)\n","\n","dummy = torch.randn(1, 3, 1024, 1280).to(device)\n","with torch.no_grad():\n","    out = model(dummy)\n","print(\"Dummy out shape:\", out.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RwOQFuftDugu","outputId":"8f87fd24-934e-4a88-826a-88b48c6a8680","executionInfo":{"status":"ok","timestamp":1765057608645,"user_tz":360,"elapsed":14227,"user":{"displayName":"pallavi sharma","userId":"07344016866799049195"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dummy out shape: torch.Size([1, 1, 1024, 1280])\n"]}]},{"cell_type":"code","source":["def masked_l1_loss(pred, target):\n","    \"\"\"\n","    pred, target: [B,1,H,W]\n","    Only compute L1 on valid disparity (target > 0)\n","    \"\"\"\n","    mask = target > 0\n","\n","    if mask.sum() == 0:\n","        # no valid pixels → return zero but keep gradient path\n","        return (pred - target).mean() * 0.0\n","\n","    return torch.abs(pred[mask] - target[mask]).mean()\n","\n","\n","LR = 1e-4\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","print(\"Loss + optimizer initialized, LR =\", LR)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsyNuzAcEn0m","outputId":"afed8588-513f-40f3-e987-d04d47ae1f0e","executionInfo":{"status":"ok","timestamp":1765057615377,"user_tz":360,"elapsed":6727,"user":{"displayName":"pallavi sharma","userId":"07344016866799049195"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss + optimizer initialized, LR = 0.0001\n"]}]},{"cell_type":"code","source":["import time\n","\n","EPOCHS = 7\n","best_val = float(\"inf\")\n","save_path = \"/content/unet_forecast_kf1to3.pth\"\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    train_loss = 0.0\n","\n","    start_time = time.time()\n","\n","    for ctx, tgt in train_loader:\n","        ctx = ctx.to(device)\n","        tgt = tgt.to(device)\n","\n","        pred = model(ctx)\n","        loss = masked_l1_loss(pred, tgt)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for ctx, tgt in val_loader:\n","            ctx = ctx.to(device)\n","            tgt = tgt.to(device)\n","            pred = model(ctx)\n","            val_loss += masked_l1_loss(pred, tgt).item()\n","\n","    val_loss /= len(val_loader)\n","\n","    epoch_time = time.time() - start_time\n","\n","    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | \"\n","          f\"Val Loss: {val_loss:.4f} | Time: {epoch_time:.1f}s\")\n","\n","    # save best model\n","    if val_loss < best_val:\n","        best_val = val_loss\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"  -> Saved best model to {save_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_85yXKeUEqgq","outputId":"b04730e6-c423-4f26-9d62-2226f4a01493","executionInfo":{"status":"ok","timestamp":1765018374950,"user_tz":360,"elapsed":18703373,"user":{"displayName":"Kesav Nagendra","userId":"14572626763742737951"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 01 | Train Loss: 2.7200 | Val Loss: 2.5598 | Time: 3481.8s\n","  -> Saved best model to /content/unet_forecast_kf1to3.pth\n","Epoch 02 | Train Loss: 1.4476 | Val Loss: 1.6169 | Time: 2555.0s\n","  -> Saved best model to /content/unet_forecast_kf1to3.pth\n","Epoch 03 | Train Loss: 1.3997 | Val Loss: 1.2574 | Time: 2548.2s\n","  -> Saved best model to /content/unet_forecast_kf1to3.pth\n","Epoch 04 | Train Loss: 1.3399 | Val Loss: 1.5327 | Time: 2540.4s\n","Epoch 05 | Train Loss: 1.4732 | Val Loss: 1.6378 | Time: 2527.1s\n","Epoch 06 | Train Loss: 1.4247 | Val Loss: 1.3590 | Time: 2522.3s\n","Epoch 07 | Train Loss: 1.3888 | Val Loss: 1.2169 | Time: 2528.5s\n","  -> Saved best model to /content/unet_forecast_kf1to3.pth\n"]}]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = UNet(in_channels=3, out_channels=1).to(device)\n","\n","CKPT_PATH = \"/content/drive/Shareddrives/TissueMotionForecasting/models/unet_forecast_kf1to3_7_epochs.pth\"  # adjust if saved elsewhere\n","state_dict = torch.load(CKPT_PATH, map_location=device)\n","model.load_state_dict(state_dict)\n","model.to(device)\n","model.eval()\n","\n","print(\"Loaded UNet checkpoint and set to eval().\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBiOwiqUDvdG","executionInfo":{"status":"ok","timestamp":1765057617281,"user_tz":360,"elapsed":1913,"user":{"displayName":"pallavi sharma","userId":"07344016866799049195"}},"outputId":"79a71385-6ed3-44c3-ae42-4ea2da8f8651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded UNet checkpoint and set to eval().\n"]}]},{"cell_type":"code","source":["import imageio.v2 as imageio\n","import numpy as np\n","import matplotlib.cm as cm\n","import cv2\n","from pathlib import Path\n","\n","SCALE_FACTOR = DISP_SCALE\n","\n","def load_disp_float(path):\n","    \"\"\"Load uint16 disparity PNG -> float (already /256).\"\"\"\n","    disp_u16 = imageio.imread(path)\n","    disp = disp_u16.astype(np.float32) / SCALE_FACTOR\n","    return disp\n","\n","def disp_to_raft_color_fixed_range(disp_float, vmin, vmax, valid_mask):\n","    \"\"\"\n","    Apply RAFT-style turbo colormap with fixed vmin/vmax and GT-based mask.\n","    disp_float: [H,W] float\n","    valid_mask: [H,W] bool\n","    \"\"\"\n","    d = np.asarray(disp_float, dtype=np.float32)\n","    d_clipped = np.clip(d, vmin, vmax)\n","    d_norm = (d_clipped - vmin) / (vmax - vmin + 1e-8)\n","    d_norm[~valid_mask] = np.nan\n","\n","    turbo = cm.get_cmap(\"turbo\")\n","    colored = turbo(d_norm)[:, :, :3]\n","    colored = np.nan_to_num(colored) * 255.0\n","    return colored.astype(np.uint8)\n"],"metadata":{"id":"3VAq0cDMO7Pq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CONTEXT_LEN = 3\n","HORIZON = 5\n","\n","DATA_ROOT = BASE_DIR / \"scared_data\" / \"train\" / \"dataset_4\"\n","kf_id = 1\n","kf_dir = DATA_ROOT / f\"keyframe_{kf_id}\"\n","rgb_path = kf_dir / \"data\" / \"rgb.mp4\"\n","disp_dir = kf_dir / \"data\" / \"disparity\"\n","\n","print(\"Keyframe dir:\", kf_dir)\n","print(\"RGB:\", rgb_path)\n","print(\"Disp dir:\", disp_dir)\n","\n","# disparity PNGs (raw only)\n","disp_paths = sorted(\n","    p for p in disp_dir.glob(\"*.png\")\n","    if not p.name.startswith(\"colored\")\n",")\n","print(\"Num disparity frames:\", len(disp_paths))\n","\n","# RGB video reader (stacked vertically: top=left, bottom=right)\n","rgb_reader = imageio.get_reader(str(rgb_path), \"ffmpeg\")\n","num_rgb_frames = rgb_reader.count_frames()\n","print(\"Num RGB frames:\", num_rgb_frames)\n","\n","# Output video writer\n","out_dir = BASE_DIR / \"videos\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","out_path = out_dir / \"dt4_kf1_left-right_gt-pred_disp_unet_7epochs.mp4\"\n","writer = imageio.get_writer(str(out_path), fps=10)\n","print(\"Writing to:\", out_path)\n","\n","model.eval()\n","\n","# t must allow t-2, t-1, t and t+hto exist in both disparity and rgb\n","max_t = min(len(disp_paths), num_rgb_frames) - HORIZON\n","start_t = CONTEXT_LEN - 1\n","\n","for t in range(start_t, max_t):\n","    rgb_frame = rgb_reader.get_data(t)\n","    H_rgb, W_rgb, _ = rgb_frame.shape\n","    mid = H_rgb // 2\n","    left_t  = rgb_frame[:mid, :, :]\n","    right_t = rgb_frame[mid:, :, :]\n","\n","\n","    ctx_indices = [t - 2, t - 1, t]\n","    ctx_disps = [load_disp_float(disp_paths[i]) for i in ctx_indices]  # list of [Hc,Wc]\n","    ctx_arr = np.stack(ctx_disps, axis=0)                               # [3,Hc,Wc]\n","    ctx_tensor = torch.from_numpy(ctx_arr).unsqueeze(0).float().to(device)\n","\n","    tgt_idx = t + HORIZON\n","    gt_disp = load_disp_float(disp_paths[tgt_idx])   # [Hc,Wc]\n","    valid_mask = gt_disp > 0\n","\n","    with torch.no_grad():\n","        pred_tensor = model(ctx_tensor)         # [1,1,Hc,Wc]\n","    pred_disp = pred_tensor[0, 0].cpu().numpy()\n","\n","\n","    gt_vals = gt_disp[valid_mask]\n","    vmin, vmax = np.percentile(gt_vals, (5, 95))\n","    if vmax <= vmin:\n","        vmax = vmin + 1e-6\n","\n","    gt_color   = disp_to_raft_color_fixed_range(gt_disp,   vmin, vmax, valid_mask)\n","    pred_color = disp_to_raft_color_fixed_range(pred_disp, vmin, vmax, valid_mask)\n","\n","    Hc, Wc, _ = gt_color.shape\n","    target_h = left_t.shape[0]\n","    scale = target_h / float(Hc)\n","    new_w = int(Wc * scale)\n","\n","    gt_color_res   = cv2.resize(gt_color,   (new_w, target_h), interpolation=cv2.INTER_LINEAR)\n","    pred_color_res = cv2.resize(pred_color, (new_w, target_h), interpolation=cv2.INTER_LINEAR)\n","\n","    left_panel  = left_t.astype(np.uint8)\n","    right_panel = right_t.astype(np.uint8)\n","\n","    frame_out = np.concatenate(\n","        [left_panel, right_panel, gt_color_res, pred_color_res],\n","        axis=1\n","    )\n","\n","    writer.append_data(frame_out)\n","\n","writer.close()\n","rgb_reader.close()\n","print(\"Done, video saved at:\", out_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sqt-JJVNyLy4","executionInfo":{"status":"ok","timestamp":1765067646843,"user_tz":360,"elapsed":9998449,"user":{"displayName":"pallavi sharma","userId":"07344016866799049195"}},"outputId":"7a305ca3-68c8-473a-b246-d55b148eafc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Keyframe dir: /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_4/keyframe_1\n","RGB: /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_4/keyframe_1/data/rgb.mp4\n","Disp dir: /content/drive/Shareddrives/TissueMotionForecasting/scared_data/train/dataset_4/keyframe_1/data/disparity\n","Num disparity frames: 728\n","Num RGB frames: 728\n","Writing to: /content/drive/Shareddrives/TissueMotionForecasting/videos/dt4_kf1_left-right_gt-pred_disp_unet_7epochs.mp4\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1704566673.py:26: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n","  turbo = cm.get_cmap(\"turbo\")\n"]},{"output_type":"stream","name":"stdout","text":["Done, video saved at: /content/drive/Shareddrives/TissueMotionForecasting/videos/dt4_kf1_left-right_gt-pred_disp_unet_7epochs.mp4\n"]}]}]}